\documentclass{article}
\usepackage[a4paper, margin=0.1in]{geometry}
\usepackage{paracol} 
\usepackage{lipsum} 
\usepackage{amsmath, amssymb, bm}
\hfuzz=10pt
\hbadness=10000  
\setlength{\columnsep}{0.5cm}
\setlength{\columnseprule}{0.4pt} 

\begin{document}

\begin{center}
\textbf{\Large Exam 1 Cheat Sheet}
\end{center}

\vspace{1cm} 

\begin{paracol}{3} 

% Column 1
\section*{Basic Probability}
\begin{enumerate}
  \item \textbf{Bayes' Theorem}
    \begin{align*}
      P(X|Y) &= \frac{P(X \cap Y)}{P(Y)}\\
             &= \frac{P(Y|X)P(X)}{P(Y)}
    \end{align*}
  \item \textbf{LOTP}
    \begin{align*}
      P[X] = \sum^{n}_{k} P[Y_k]P[X|Y_k]
    \end{align*}
  \item \textbf{Expectation}
    \\
    For $T$ being some linear transformation of $X$
    \begin{align*}
      E[X] &= \int xP[X] dx\\
      E[T] &= \int T[x] P[X] dx \\
      Var[X] &= E[(X - \mu)^2] \\
             &= E[X^2] - E[X]^2 
    \end{align*}
  \item \textbf{Total Variance}
    \[
      \text{Var}[X] = E[\text{Var}[X|Y]] + \text{Var}[E[X|Y]]
    \]
  \item \textbf{Double Expectation}
    \\
    Given, $P[X|Y] \text{ and } P[Y]$
    \begin{align*}
      P[X] &= E[P[X|Y]]\\
           &= \int P[X|Y = b]P[Y=b]db \\
      E[X] &= E[E[X|Y]]\\
           &= \int E[X|Y = b]P[Y=b]db 
    \end{align*}
  \item \textbf{Joint, Marginal, conditional}  \\
    given $f^{(X,Y)}$ 
    \begin{align*}
      f^X &= \int f^{(X,Y)}(x,y) dy \\
      f^Y &= \int f^{(X,Y)}(x,y) dx \\
      f^{X|Y}(x \mid y) &= \frac{f(x,y)}{f^Y(y)} \\
      f^{Y|X}(y \mid x) &= \frac{f(x,y)}{f^X(x)} \\
      E[Y|X=x] &= \int y f^{Y|X}(y|x) dy \\
      f(x,y) &= f^X(x) \cdot f^{Y|X}(y \mid x)\\  
             &= f^Y(y) \cdot f^{X|Y}(x \mid y)
    \end{align*}
\end{enumerate}

\switchcolumn

\section*{Continuous Distributions}
\begin{enumerate}
      \item \textbf{Uniform } \( U(a, b) \)
    \begin{itemize}
        \item PDF:
        \[
        f(x) = \frac{1}{b - a}, \quad a \leq x \leq b
        \]
        \item Mean: \( \frac{a + b}{2} \)
        \item Variance: \( \frac{(b - a)^2}{12} \)
    \end{itemize}

    \item \textbf{Exponential } \( \text{Exp}(\lambda) \)
    \begin{itemize}
        \item PDF:
        \[
        f(x) = \lambda e^{-\lambda x}, \quad x \geq 0
        \]
        \item Mean: \( \frac{1}{\lambda} \)
        \item Variance: \( \frac{1}{\lambda^2} \)
    \end{itemize}

  \item \textbf{Gamma} \( \text{Gamma}(a, \theta = \frac{1}{\lambda}) \)
    \begin{itemize}
        \item PDF:
        \[
        f(x) = \frac{\lambda^a x^{a-1} e^{-\lambda x}}{\Gamma(a)}, \quad x \geq 0
        \]
        \item Mean: \( \frac{a}{\lambda} \)
        \item Variance: \( \frac{a}{\lambda^2} \)
    \end{itemize}

    \item \textbf{Beta } \( \text{Beta}(\alpha, \beta) \)
    \begin{itemize}
        \item PDF:
        \[
        f(x) = \frac{x^{\alpha - 1} (1 - x)^{\beta - 1}}{B(\alpha, \beta)}
        \]
        \item Mean: \( \frac{\alpha}{\alpha + \beta} \)
        \item Variance: \( \frac{\alpha \beta}{(\alpha + \beta)^2 (\alpha + \beta + 1)} \)
    \end{itemize}
    \item \textbf{Beta and Gamma integrals} 
    \begin{itemize}
      \item Gamma
      \[
      \Gamma(x) = \int_0^\infty t^{x-1} e^{-t} \, dt, \quad x > 0
      \]
    \item Beta
      \begin{align*}
        B(x, y) &= \int_0^1 t^{x-1} (1 - t)^{y-1}dt \\
                &= \frac{\Gamma(x) \Gamma(y)}{\Gamma(x + y)} 
      \end{align*}
    \end{itemize}
\end{enumerate}
\switchcolumn 
\section*{Common Moments}
\begin{enumerate}
  \item 
    $$ 
    1 - \text{Beta}[\alpha, \beta] = \text{Beta}[\beta, \alpha]
    $$
  \item Moments of Gamma Distribution
    \begin{itemize}
      \item Reciprocal:
        \begin{align*}
        \mathbb{E} \left[ \frac{1}{X} \right] &= \frac{\lambda}{(a-1)}
        \end{align*}
      \item Second Reciprocal:
        \begin{align*}
        \mathbb{E} \left[ \frac{1}{X^2} \right] &= \frac{\lambda^2}{(a-1)(a-2)}
        \end{align*}
    \end{itemize} 
  \item Moments of Beta
    \begin{itemize}
      \item Reciprocal:
        \begin{align*}
        \mathbb{E} \left[ \frac{1}{Q} \right] &= \frac{\beta}{\alpha - 1} + 1
        \end{align*}
      \item Odds Ratio:
        \begin{align*}
        \mathbb{E} \left[ \frac{1 - Q}{Q} \right] &= \frac{\beta}{\alpha - 1}
        \end{align*}
      \item Second Odds:
        \begin{align*}
        \mathbb{E} \left[ \frac{(1 - Q)^2}{Q^2} \right] &= \frac{\beta \cdot (\beta + 1)}{(\alpha - 1) \cdot (\alpha - 2)}
        \end{align*}
      \item Variance of Odds:
        \begin{align*}
        \text{Var} \left[\frac{1-Q}{Q} \right] &= \frac{\beta(\alpha+\beta-1)}{(\alpha-1)^2(\alpha-2)}
        \end{align*}
    \end{itemize}
\end{enumerate}
\section*{random sum}
\begin{enumerate}
  \item given discrete random variable $n$, and a group of i.i.d. continous random variables $x_j$, then the random sum $s = \sum^{n} x_j$ has the following moments
  \break
  \[
    e[s] = \mu \cdot \nu ~ \text{var}[s] = \mu^2 \cdot \tau^2 + \nu \cdot \sigma^2
  \]
  where:
  \begin{align*}
    \mu = e[x], \sigma^2 &= var[x],\nu = e[n]\\
    \tau^2 &= \text{var}[n]
  \end{align*}
\end{enumerate}
\end{paracol}
\newpage
\begin{paracol}{3}
\section*{Change of Variables}
\begin{enumerate}
  \item Linear change of variables: if $X$ is an RV, and $Y = aX+b$ 
    \[
      f^Y(y) = \frac{1}{a}f^X(\frac{y-b}{a}) 
    \]
  \item reciporcal distribution: let $Y = X^{-1}$
    \[
      f^Y(y) = \frac{1}{y^2} f^X(\frac{1}{y})
    \]
  \item Generally, the p.d.f follows the Jacobian Change of variables formula, for $Y = g(X)$
    \[
      f^Y(y) = |\frac{d}{dy}g^{-1}(y)|f^X(g^{-1}(x))
    \]
\end{enumerate}
\section*{Geometric}
\begin{enumerate}
  \item $X$ is geometric with support $\mathbb{N}$:
    \begin{align*}
      Pr(X = k) &= (1-p)^{k-1}p \\
      E[X] &= \frac{1}{p}
    \end{align*}
  \item Y is geometric with support $\mathbb{N_0}$:
    \begin{align*}
      Pr(Y = k) &= (1-p)^{k-1}p \\
      E[Y] &= \frac{1-p}{p}
    \end{align*}
  \item Both share the same variance $\frac{1-p}{p^2}$
  \item if $Q \sim \text{Beta}[\alpha, \beta]$, and $(N|Q = q) \sim \text{Geom}[q;0]$ (that is $0$ is included in the support.
    \begin{align*}
      E[N] &= E[\frac{1-Q}{Q}]= \frac{\beta}{\alpha - 1} \\
      E[N^2] &= \frac{\beta(\beta + 1)}{(\alpha -1)(\alpha -2)}
    \end{align*}
  \item if $M | Q = q \sim \text{Geom}[q;0]$
    \begin{align*}
      E[M] &= E[\frac{1}{Q}] = \frac{\alpha + \beta - 1}{\alpha - 1}
    \end{align*}
\end{enumerate}
\section*{Other useful things}
\begin{enumerate}
  \item $Y \sim \text{Gamma}[a, \theta]$ if $W | Y = y \sim \text{Gamma}[1, \frac{1}{y}]$
    \begin{align*}
      E[W] &= E[\frac{1}{Y}] = \frac{\theta^{-1}}{a-1} \\
      E[W^2] &= 2\frac{\theta^{-2}}{(a-1)(a-2)} \\
      Y | W &= w \sim \text{Gamma}[a+1,\frac{1}{\theta^{-1}+w}]
    \end{align*}
  \item with the same $Y$, if $N | Y = y \sim \text{Pois}[y]$ then
    \begin{align*}
      E[N] = E[Y] &= a \cdot \theta \\
      E[N|Y] &= y\\
      Var[N] &= E[Y] + Var[Y] \\
      Y | N = n &\sim Gamma[a + n; \frac{1}{\lambda + 1}
    \end{align*}
\end{enumerate}
\end{paracol} 
\end{document}

