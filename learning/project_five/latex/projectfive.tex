\documentclass[11pt]{article}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{latexsym}
\usepackage{enumerate}
\usepackage{minted}
\usepackage{afterpage}
\usepackage{placeins}
\usepackage{needspace}
\usepackage{float}
\usepackage{url}
\usepackage[margin=0.75in]{geometry}
\usepackage{mathtools}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
\begin{center}
	%% put the project number
	\textbf{STAT 4360 (Introduction to Statistical Learning, Fall 2022)
		\\[1ex]
		Mini Project 5 
		\\[1ex]
		Name: Nimrod Ohayon Rozanes}
	\\
	{\_}\hrulefill{\hspace{.01in}}
\end{center}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Section 1: 

\begin{enumerate}
\item ~
	\begin{enumerate}
		\item When performing a principle component analysis, it is necessary to standardize the predictors since the PCA maximizes the variance of each created principle component. As such when we standardize the predictors, each variable contributes equally, so a predictor on a larger scale will not dominate the analysis.
		\item Following is the Scree Plot for the PCA 
			\begin{figure}[htbp]
				\centering
				\includegraphics[width=0.8\textwidth]{scree.png}
			\end{figure}
    The elbow of the of the PCA lays in the range of $4-7$, with the least amount that I would likely use being $M = 5$
    \pagebreak
    \item Following is also the biplot for the biplot for a PCA with $M=2$. As well as the loadings of each predictor and the principle components
			\begin{figure}[htbp]
				\centering
				\includegraphics[width=0.6\textwidth]{biplot.png}
			\end{figure}
      \begin{table}[htbp]
      \centering
        \caption{PCA Loadings for PC1 and PC2}
        \begin{tabular}{lrr}
        \toprule
        \textbf{Feature} & \textbf{PC1} & \textbf{PC2} \\
        \midrule
        AtBat    &  0.196209 &  0.391118 \\
        Hits     &  0.193644 &  0.383922 \\
        HmRun    &  0.201534 &  0.235943 \\
        Runs     &  0.195226 &  0.381179 \\
        RBI      &  0.232769 &  0.318169 \\
        Walks    &  0.208483 &  0.237312 \\
        Years    &  0.285183 & -0.259716 \\
        CAtBat   &  0.333319 & -0.186708 \\
        CHits    &  0.333577 & -0.176440 \\
        CHmRun   &  0.320087 & -0.124551 \\
        CRuns    &  0.340501 & -0.167246 \\
        CRBI     &  0.342675 & -0.163078 \\
        CWalks   &  0.319261 & -0.187581 \\
        PutOuts  &  0.078227 &  0.163404 \\
        Assists  & -0.000531 &  0.176953 \\
        Errors   & -0.007283 &  0.210699 \\
        \bottomrule
        \end{tabular}
        \end{table}
    Visually, the biplot shows two clusters of predictors that correlate with PC1 and PC2. The first, correlating with PC1, are CAtBat, CHits, CRBI, CRuns, which are all cumulative statistics about the player. Notably, PC1 does not have any high magnitude negative loadings. PC2 on the other hand has high positive loadings with the season specific statistics of a player, AtBat, RBI, etc. with negative loadings on the cumulative statistics, which could be interpreted as contrasting the players performance in this season vs high career averages.
    \end{enumerate}
    \item ~
    \begin{enumerate}
        \item Following is a table of the type of regression, any meta variables chosen, and the MSE of the that variable 
        \begin{table}[htbp]
        \centering
        \caption{Sample Three-Column Table}
        \begin{tabular}{l c r}
        \toprule
        \textbf{Regression} & \textbf{Parameter} & \textbf{LOOCV MSE} \\
        \midrule
        OLS & N/A & 0.0804 \\
        PCR & $M=13$ & 0.079 \\
        PLS & $M=10$ & 0.079\\
        Ridge & $\alpha=13.22$ & 0.078 \\
        \bottomrule
        \end{tabular}
        \label{tab:sample}
        \end{table}
        All of the above regressions have similar MSE's, suggesting that the assumptions of the OLS linear regression are likely to be true. However, I would likely suggest to use the ridge regression, which has the lowest MSE, but also because it handles multicolinearity the best, which is relevant since this data set in particular has several predictors which have correlating other predictors that they would be linear with (cRBI with RBI, for example). Both PCR and PLS also address multicolinearlity, but on merit of the Ridge regression's lower MSE, I believe it is the most suitable regression for this data.
    \end{enumerate}	
    \item ~ 
        \begin{enumerate}
        \item After fitting a full linear regression to the data, the p-values of the predictors follow:
            \begin{center}
            \begin{tabular}{lclc}
            \toprule
            \textbf{Dep. Variable:}    &      Salary      & \textbf{  R-squared:         } &     0.543   \\
            \textbf{Model:}            &       OLS        & \textbf{  Adj. R-squared:    } &     0.513   \\
            \textbf{Method:}           &  Least Squares   & \textbf{  F-statistic:       } &     18.25   \\
            \textbf{Date:}             & Thu, 24 Apr 2025 & \textbf{  Prob (F-statistic):} &  2.47e-33   \\
            \textbf{Time:}             &     17:18:48     & \textbf{  Log-Likelihood:    } &   -19.558   \\
            \textbf{No. Observations:} &         263      & \textbf{  AIC:               } &     73.12   \\
            \textbf{Df Residuals:}     &         246      & \textbf{  BIC:               } &     133.8   \\
            \textbf{Df Model:}         &          16      & \textbf{                     } &             \\
            \textbf{Covariance Type:}  &    nonrobust     & \textbf{                     } &             \\
            \bottomrule
            \end{tabular}
            \begin{tabular}{lcccccc}
                             & \textbf{coef} & \textbf{std err} & \textbf{t} & \textbf{P$> |$t$|$} & \textbf{[0.025} & \textbf{0.975]}  \\
            \midrule
            \textbf{const}   &       2.0056  &        0.070     &    28.465  &         0.000        &        1.867    &        2.144     \\
            \textbf{AtBat}   &      -0.0015  &        0.001     &    -2.789  &         0.006        &       -0.003    &       -0.000     \\
            \textbf{Hits}    &       0.0060  &        0.002     &     2.987  &         0.003        &        0.002    &        0.010     \\
            \textbf{HmRun}   &       0.0038  &        0.005     &     0.724  &         0.470        &       -0.007    &        0.014     \\
            \textbf{Runs}    &      -0.0006  &        0.003     &    -0.245  &         0.807        &       -0.006    &        0.004     \\
            \textbf{RBI}     &      -0.0001  &        0.002     &    -0.063  &         0.950        &       -0.004    &        0.004     \\
            \textbf{Walks}   &       0.0048  &        0.002     &     3.098  &         0.002        &        0.002    &        0.008     \\
            \textbf{Years}   &       0.0243  &        0.010     &     2.311  &         0.022        &        0.004    &        0.045     \\
            \textbf{CAtBat}  &    6.282e-05  &        0.000     &     0.546  &         0.586        &       -0.000    &        0.000     \\
            \textbf{CHits}   &      -0.0003  &        0.001     &    -0.483  &         0.630        &       -0.001    &        0.001     \\
            \textbf{CHmRun}  &      -0.0002  &        0.001     &    -0.121  &         0.904        &       -0.003    &        0.003     \\
            \textbf{CRuns}   &       0.0008  &        0.001     &     1.207  &         0.228        &       -0.000    &        0.002     \\
            \textbf{CRBI}    &     9.54e-05  &        0.001     &     0.162  &         0.872        &       -0.001    &        0.001     \\
            \textbf{CWalks}  &      -0.0006  &        0.000     &    -2.235  &         0.026        &       -0.001    &    -7.44e-05     \\
            \textbf{PutOuts} &       0.0002  &      6.6e-05     &     2.406  &         0.017        &     2.88e-05    &        0.000     \\
            \textbf{Assists} &       0.0003  &        0.000     &     1.445  &         0.150        &     -9.9e-05    &        0.001     \\
            \textbf{Errors}  &      -0.0043  &        0.004     &    -1.166  &         0.245        &       -0.012    &        0.003     \\
            \bottomrule
            \end{tabular}
            \end{center}
            The most relevant predictors, measured by the p-value of that predictor, are AtBat, Hits, and Walks, while the career statistics have a much less substantive contribution to the pay of the player. This is qualitatively reasonable, as it makes since that an older player, who would have higher career statistics, might get paid less than a younger player. While walks does have a slightly lower p-value to hits, I am choosing hits as the most important predictor, since in an actual game of baseball walks are relatively rare, and do not reflect much about the actual player.
        \item Fitting a natural cubic spline regression to Hits vs Log(Salary), and sweeping over possible knot values, we get that the optimal number of knots is $8$ with an cross validated MSE of $0.115$, which produces the following graph
			\begin{figure}[htbp]
				\centering
          \includegraphics[width=0.8\textwidth]{cubic.png}
        \end{figure}
        \end{enumerate}
\end{enumerate}
%% Section 2: present the code
\newpage
\begin{center}
	\textbf{Python Code}
\end{center}

\hrule
\begin{minted}{python}
  #!/usr/bin/env python
# coding: utf-8

# In[43]:


from sklearn.preprocessing import StandardScaler
import pandas as pd
import numpy as np
from ISLP import load_data
std_scaler = StandardScaler()
hitters = load_data('Hitters')
numeric = hitters.select_dtypes(include='number').dropna().drop('Salary',axis=1)
scaled = std_scaler.fit_transform(numeric)


# In[44]:


from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

pca = PCA()
pca.fit(scaled)

plt.figure(figsize=(8, 5))
plt.plot(range(1, len(pca.explained_variance_ratio_) + 1),
         pca.explained_variance_ratio_,
         marker='o')
plt.title('Scree Plot')
plt.xlabel('Principal Component')
plt.ylabel('Proportion of Variance Explained')
plt.xticks(range(1, len(pca.explained_variance_ratio_) + 1))
plt.grid(True)
plt.tight_layout()
plt.savefig("scree.png")


# In[48]:


import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

pca = PCA(n_components=2)
X_pca = pca.fit_transform(scaled)

pca_df = pd.DataFrame(X_pca, columns=['PC1', 'PC2'])

loadings = pd.DataFrame(
    pca.components_.T,
    columns=['PC1', 'PC2'],
    index=numeric.columns
)

print(loadings)
plt.figure(figsize=(10, 8))
sns.set(style='whitegrid')

for i in range(loadings.shape[0]):
    plt.arrow(0, 0,
              loadings.iloc[i, 0]*3, 
              loadings.iloc[i, 1]*3,
              color='red', alpha=0.5, head_width=0.00)
    plt.text(loadings.iloc[i, 0]*3.2,
             loadings.iloc[i, 1]*3.2,
             loadings.index[i],
             color='red', ha='center', va='center')

explained_var = pca.explained_variance_ratio_ * 100
plt.xlabel(f"PC1 ({explained_var[0]:.1f}%)")
plt.ylabel(f"PC2 ({explained_var[1]:.1f}%)")
plt.title('PCA Biplot')
plt.axhline(0, color='grey', linewidth=0.5)
plt.axvline(0, color='grey', linewidth=0.5)
plt.grid(True)
plt.tight_layout()
plt.savefig("biplot.png")


# In[68]:


std_scaler = StandardScaler()
hitters = load_data('Hitters').dropna()
X = hitters.select_dtypes(include='number').drop('Salary',axis=1)
y = np.log10(hitters['Salary'])
X_scaled = std_scaler.fit_transform(numeric)


# In[64]:


import pandas as pd
import statsmodels.api as sm
from ISLP import load_data

Hitters = load_data('Hitters')
Hitters = Hitters.dropna().select_dtypes(include='number')
Hitters_encoded = pd.get_dummies(Hitters, drop_first=True)
X = Hitters_encoded.drop('Salary', axis=1)
y = np.log10(Hitters_encoded['Salary'])
X = sm.add_constant(X)
X = X.astype(float)
y = y.astype(float)
model = sm.OLS(y, X).fit()
print(model.summary().as_latex())


# In[47]:


from sklearn.linear_model import LinearRegression
from sklearn.model_selection import LeaveOneOut, cross_val_score
from sklearn.pipeline import make_pipeline
from tqdm import tqdm
reg = LinearRegression()
loo = LeaveOneOut()
mse = -cross_val_score(reg, X, y, cv=loo, scoring='neg_mean_squared_error').mean()
print(mse)


# In[49]:


mse_scores = []
for m in tqdm(range(1, 17)):
    pcr = make_pipeline(StandardScaler(), PCA(n_components=m), LinearRegression())
    scores = cross_val_score(pcr, X, y, cv=loo, scoring='neg_mean_squared_error')
    mse = -scores.mean()
    mse_scores.append(mse)

optimal = np.argmin(mse_scores) + 1
print(optimal)


# In[50]:


mse_scores[optimal - 1]


# In[22]:


from sklearn.cross_decomposition import PLSRegression
mse_scores = []
for m in tqdm(range(1, 17)):
    pls = PLSRegression(n_components=m)
    scores = cross_val_score(pls, X, y, cv=loo, scoring='neg_mean_squared_error')
    mse = -scores.mean()
    mse_scores.append(mse)

optimal = np.argmin(mse_scores) + 1
print(optimal)


# In[51]:


mse_scores[optimal - 1]


# In[29]:


mse_scores = []
alphas = np.logspace(-3, 3, 100)
loo = LeaveOneOut()

for alpha in tqdm(alphas):
    ridge = make_pipeline(StandardScaler(), Ridge(alpha=alpha))
    scores = cross_val_score(ridge, X, y, cv=loo, scoring='neg_mean_squared_error')
    mse_scores.append((-scores.mean(), alpha))

optimal = min(mse_scores)
print(f"Optimal alpha: {optimal[1]} with MSE: {optimal[0]}")


# In[32]:


from sklearn.feature_selection import f_regression

f_stat, p_values = f_regression(X, y)

# Display p-values
for idx, p_val in enumerate(p_values):
    print(f"Feature {X.columns[idx]}: p-value = {p_val}")


# In[71]:


import numpy as np
import pandas as pd
from sklearn.preprocessing import SplineTransformer
from sklearn.linear_model import LinearRegression
from sklearn.pipeline import make_pipeline
from sklearn.model_selection import LeaveOneOut, cross_val_score
from tqdm import tqdm
mse_scores = []
knot_range = range(3, 15)
X = X[["Hits"]]
for n in tqdm(knot_range):
    model = make_pipeline(
        SplineTransformer(degree=3, n_knots=n, include_bias=False),
        LinearRegression()
    ) 
    scores = cross_val_score(model, X, y, cv=loo, scoring='neg_mean_squared_error')
    mse = -scores.mean()
    mse_scores.append((mse, n))

optimal_mse, optimal_knots = min(mse_scores, key=lambda x: x[0])
print(optimal_knots)
print(optimal_mse)


# In[73]:


optimal_knots = 8  
model = make_pipeline(
    SplineTransformer(degree=3, n_knots=optimal_knots, include_bias=False),
    LinearRegression()
)
model.fit(X, y)
X_plot = np.linspace(X.min(), X.max(), 500).reshape(-1, 1)
y_plot = model.predict(X_plot)
plt.figure(figsize=(10, 6))
plt.scatter(X, y, color='gray', alpha=0.5, label='Data')
plt.plot(X_plot, y_plot, color='red', linewidth=2, label='Spline Regression Fit')
plt.xlabel('Hits')
plt.ylabel('Salary')
plt.title(f'Cubic Spline Regression with {optimal_knots} Knots')
plt.legend()
plt.tight_layout()
plt.savefig("cubic.png")
\end{minted}
\end{document}
