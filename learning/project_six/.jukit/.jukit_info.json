{"cmd": "import numpy as np\nimport pandas as pd\nfrom ISLP import load_data\nfrom pandas.core.arrays import categorical\nfrom pandas.core.common import random_state\nfrom sklearn.pipeline import FunctionTransformer, Pipeline\nfrom sklearn.tree import DecisionTreeRegressor, plot_tree\n\nHitters = pd.get_dummies(\n    pd.DataFrame(load_data(\"Hitters\")).dropna(subset=[\"Salary\"]),\n    columns=[\"League\", \"Division\", \"NewLeague\"],\n    drop_first=False,\n)\n\ntree = DecisionTreeRegressor(random_state=0)\n\nX = Hitters.drop(columns=[\"Salary\"])\ny = np.log(Hitters[\"Salary\"])\n\n\ntree.fit(X, y)\n\nimport matplotlib.pyplot as plt\nfrom sklearn.tree import export_text, plot_tree\n\n# Excessively large tree\nplt.figure(figsize=(20, 10))\nplot_tree(tree, filled=True)\nplt.show()\n\n# tree_rules = export_text(tree_model)\n\nfrom sklearn.model_selection import LeaveOneOut, cross_val_score\nfrom tqdm import tqdm\n\ncv_mse = -np.mean(\n    cross_val_score(\n        tree, X, y, cv=LeaveOneOut(), scoring=\"neg_mean_squared_error\", n_jobs=-1\n    )\n)\n\nprint(cv_mse)\n\n# b)\ntree = DecisionTreeRegressor(random_state=0)\npath = tree.cost_complexity_pruning_path(X, y)\nalphas, impurities = path.ccp_alphas, path.impurities\n\ntrees = []\nfor alpha in tqdm(alphas):\n    regressor = DecisionTreeRegressor(random_state=0, ccp_alpha=alpha)\n    regressor.fit(X, y)\n    trees.append(regressor)\n\n\nloo = LeaveOneOut()\nmse_scores = []\ncounts = []\nfor regressor in tqdm(trees):\n    scores = cross_val_score(\n        regressor, X, y, cv=loo, scoring=\"neg_mean_squared_error\", n_jobs=-1\n    )\n    mse_scores.append(-scores.mean())\n    counts.append(regressor.tree_.node_count)\n\noptimal_index = np.argmin(mse_scores)\noptimal_alpha = alphas[optimal_index]\nprint(f\"Optimal ccp_alpha: {optimal_alpha}\")\noptimal_tree = DecisionTreeRegressor(random_state=0, ccp_alpha=optimal_alpha)\noptimal_tree.fit(X, y)\nfrom sklearn.tree import plot_tree\n\nplt.figure(figsize=(20, 10))\nplot_tree(optimal_tree, feature_names=X.columns, filled=True)\nplt.title(\"Pruned Decision Tree\")\nplt.savefig(\"pruned.png\")\nplt.figure(figsize=(8, 6))\n\nplt.plot(alphas, mse_scores, marker=\"o\", drawstyle=\"steps-post\")\nplt.xlabel(\"ccp_alpha\")\nplt.ylabel(\"LOOCV Mean Squared Error\")\nplt.title(\"LOOCV MSE vs. ccp_alpha\")\nplt.grid(True)\nplt.savefig(\"prunedMSEvsAlpha.png\")\n\nplt.figure(figsize=(8, 6))\nplt.plot(counts, mse_scores, marker=\"o\")\nplt.xlabel(\"Number of Nodes in Tree\")\nplt.ylabel(\"LOOCV Mean Squared Error\")\nplt.title(\"Tree Size vs LOOCV MSE\")\nplt.grid(True)\nplt.tight_layout()\nplt.savefig(\"prunedSizevsMSE.png\")\n\n# Extract feature importances\nimportances = optimal_tree.feature_importances_\nfeature_names = X.columns\n\n# Sort the feature importances in descending order\nindices = np.argsort(importances)[::-1]\n\n# Plot the feature importances\nplt.figure(figsize=(10, 6))\nplt.title(\"Feature Importances - Pruned Decision Tree\")\nplt.bar(range(X.shape[1]), importances[indices], align=\"center\")\nplt.xticks(range(X.shape[1]), [feature_names[i] for i in indices], rotation=90)\nplt.xlabel(\"Features\")\nplt.ylabel(\"Importance\")\nplt.tight_layout()\nplt.savefig(\"prunedimportance.png\")\n\n\n# c)\nfrom sklearn.ensemble import BaggingRegressor\n\nB = 1000\nbagg = BaggingRegressor(n_estimators=B, random_state=0).fit(X, y)\ncv_mse = -np.mean(\n    cross_val_score(\n        bagg, X, y, cv=LeaveOneOut(), scoring=\"neg_mean_squared_error\", n_jobs=-1\n    )\n)\nprint(cv_mse)\n# Ensure that the base estimator exposes feature_importances_\nimportances = np.mean([tree.feature_importances_ for tree in bagg.estimators_], axis=0)\n\n# Sort the feature importances in descending order\nindices = np.argsort(importances)[::-1]\n\n# Plot the feature importances\nplt.figure(figsize=(10, 6))\nplt.title(\"Feature Importances - Bagging Regressor\")\nplt.bar(range(X.shape[1]), importances[indices], align=\"center\")\nplt.xticks(range(X.shape[1]), [feature_names[i] for i in indices], rotation=90)\nplt.xlabel(\"Features\")\nplt.ylabel(\"Importance\")\nplt.tight_layout()\nplt.savefig(\"baggingImportance.png\")\n\n# d\nfrom sklearn.ensemble import RandomForestRegressor\n\nrandom = RandomForestRegressor(\n    n_estimators=B, max_depth=int(np.floor(len(X.columns) / 3)), random_state=0\n)\ncv_mse = -np.mean(\n    cross_val_score(\n        random, X, y, cv=LeaveOneOut(), scoring=\"neg_mean_squared_error\", n_jobs=-1\n    )\n)\nprint(cv_mse)\n# Extract feature importances\nimportances = random.feature_importances_\n\n# Sort the feature importances in descending order\nindices = np.argsort(importances)[::-1]\n\n# Plot the feature importances\nplt.figure(figsize=(10, 6))\nplt.title(\"Feature Importances - Random Forest Regressor\")\nplt.bar(range(X.shape[1]), importances[indices], align=\"center\")\nplt.xticks(range(X.shape[1]), [feature_names[i] for i in indices], rotation=90)\nplt.xlabel(\"Features\")\nplt.ylabel(\"Importance\")\nplt.tight_layout()\nplt.savefig(\"randomImportance.png\")\n\n# e\n\nfrom sklearn.ensemble import GradientBoostingRegressor\n\nboost = GradientBoostingRegressor(n_estimators=B, learning_rate=0.01, max_depth=1)\ncv_mse = -np.mean(\n    cross_val_score(\n        boost, X, y, cv=LeaveOneOut(), scoring=\"neg_mean_squared_error\", n_jobs=-1\n    )\n)\nprint(cv_mse)\n# Extract feature importances\nimportances = boost.feature_importances_\n\n# Sort the feature importances in descending order\nindices = np.argsort(importances)[::-1]\n\n# Plot the feature importances\nplt.figure(figsize=(10, 6))\nplt.title(\"Feature Importances - Gradient Boosting Regressor\")\nplt.bar(range(X.shape[1]), importances[indices], align=\"center\")\nplt.xticks(range(X.shape[1]), [feature_names[i] for i in indices], rotation=90)\nplt.xlabel(\"Features\")\nplt.ylabel(\"Importance\")\nplt.tight_layout()\nplt.savefig(\"boostedImportance.png\")\n\n\n# 2)\ndiabetes = pd.read_csv(\"diabetes.csv\")\ndiabetes.columns = diabetes.columns.str.replace(\" \", \"\")\ndiabetes.columns = diabetes.columns.str.replace(\"\\n\", \"\")\n\nfrom sklearn.model_selection import GridSearchCV, KFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\n\nX = diabetes.drop(columns=\"Outcome\")\ny = diabetes[\"Outcome\"]\n\nscaler = StandardScaler()\nscaler.fit_transform(X)\n\nc_logspace = np.logspace(-2, 2, num=15)  # from 0.01 to 100\ngamma_values = np.logspace(-15, 3, num=19, base=2)\n\nsvc = SVC(kernel=\"linear\")\ncv = KFold(n_splits=10)\n\ngrid_search = GridSearchCV(\n    svc, c_logspace, scoring=\"neg_mean_squared_error\", cv=cv, n_jobs=-1\n)\n\nbest_params = grid_search.best_params_\nbest_score = -grid_search.best_score_\n\noptim_svm = SVC(kernel=\"linear\", C=best_params).fit(X, y)\ncoefs = optim_svm.coef_[0]\nfeature_names = X.columns\nplt.figure(figsize=(10, 6))\nplt.barh(feature_names, coefs)\nplt.xlabel(\"Coefficient Value\")\nplt.title(\"Feature Importance in Linear SVC\")\nplt.show()\n\nsvm = SVC(kernel=\"poly\", degree=2)\ngrid_search = GridSearchCV(\n    svm, c_logspace, scoring=\"neg_mean_squared_error\", cv=cv, n_jobs=-1\n)\n\nbest_params = grid_search.best_params_\nbest_score = -grid_search.best_score_\n\noptim_svm = SVC(kernel=\"poly\", degree=2, C=best_params).fit(X, y)\ncoefs = optim_svm.coef_[0]\nfeature_names = X.columns\nplt.figure(figsize=(10, 6))\nplt.barh(feature_names, coefs)\nplt.xlabel(\"Coefficient Value\")\nplt.title(\"Feature Importance in Linear SVC\")\nplt.show()\n\nparam_grid = {\"C\": c_logspace, \"gamma\": gamma_values}\n\nsvm = SVC(kernel=\"rbf\")\ngrid_search = GridSearchCV(\n    svm, param_grid, scoring=\"neg_mean_squared_error\", cv=cv, n_jobs=-1\n)\n\nbest_params = grid_search.best_params_\n\noptim_svm = SVC(kernel=\"rbf\", gamma=best_params[\"gamma\"], C=best_params[\"C\"]).fit(X, y)\ncoefs = optim_svm.coef_[0]\nfeature_names = X.columns\nplt.figure(figsize=(10, 6))\nplt.barh(feature_names, coefs)\nplt.xlabel(\"Coefficient Value\")\nplt.title(\"Feature Importance in Linear SVC\")\nplt.show()\n\n\n# 3)\nfrom scipy.cluster.hierarchy import dendrogram\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.preprocessing import StandardScaler\n\nHitters = pd.get_dummies(\n    pd.DataFrame(load_data(\"Hitters\")).dropna(subset=[\"Salary\"]),\n    columns=[\"League\", \"Division\", \"NewLeague\"],\n    drop_first=False,\n)\n\nstd = StandardScaler()\nstd.fit_transform(Hitters)\nHitters[\"Salary\"] = np.log(Hitters[\"Salary\"])\n\nmodel = AgglomerativeClustering(\n    n_clusters=None, distance_threshold=0, linkage=\"complete\", metric=\"euclidean\"\n).fit(Hitters)\n\n\ndef plot_dendrogram(model, **kwargs):\n    counts = np.zeros(model.children_.shape[0])\n    n_samples = len(model.labels_)\n    for i, merge in enumerate(model.children_):\n        current_count = 0\n        for child_idx in merge:\n            if child_idx < n_samples:\n                current_count += 1\n            else:\n                current_count += counts[child_idx - n_samples]\n        counts[i] = current_count\n\n    linkage_matrix = np.column_stack(\n        [model.children_, model.distances_, counts]\n    ).astype(float)\n\n    dendrogram(linkage_matrix, **kwargs)\n\n\nplt.title(\"Hierarchical Clustering Dendrogram\")\nplot_dendrogram(model, truncate_mode=\"level\", p=6)\nplt.xlabel(\"Number of points in node (or index of point if no parenthesis).\")\nplt.show()\n\nfrom scipy.cluster.hierarchy import fcluster, linkage\n\nZ = linkage(Hitters, method=\"complete\")\nlabels = fcluster(Z, t=2, criterion=\"maxclust\")\ncluster_means = []\nfor cluster_id in np.unique(labels):\n    cluster_data = Hitters[labels == cluster_id]\n    cluster_mean = np.mean(cluster_data, axis=0)\n    cluster_means.append(cluster_mean)\n\n\ncluster_means = pd.DataFrame(np.array(cluster_means), columns=Hitters.columns)\n\ncluster_means\n\n# d)\nfrom sklearn.cluster import KMeans\n\nkmeans = KMeans(n_clusters=2, random_state=0, n_init=\"auto\").fit(Hitters)\ncluster_means = pd.DataFrame(kmeans.cluster_centers_, columns=Hitters.columns)", "cmd_opts": "", "import_complete": 1, "terminal": "nvimterm"}