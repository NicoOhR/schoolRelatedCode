\documentclass[11pt]{article}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{latexsym}
\usepackage{enumerate}
\usepackage{minted}
\usepackage{afterpage}
\usepackage{placeins}
\usepackage{needspace}
\usepackage{float}
\usepackage{url}
\usepackage[margin=0.75in]{geometry}
\usepackage{mathtools}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
\begin{center}
	%% put the project number
	\textbf{STAT 4360 (Introduction to Statistical Learning, Fall 2022)
		\\[1ex]
		Mini Project 3 
		\\[1ex]
		Name: Nimrod Ohayon Rozanes}
	\\
	{\_}\hrulefill{\hspace{.01in}}
\end{center}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Section 1: 

\begin{enumerate}
\item Question 1
	\begin{enumerate}
			\item ~
			\begin{table}[h]
			\centering
			\begin{tabular}{|c|c|c|}
			\hline
			\textbf{Error} & \textbf{Specificity} & \textbf{Sensitivity} \\
			\hline
			0.217 & 0.738& 0.799\\
			\hline
			\end{tabular}
			\caption{Error, Specificity, and Sensitivity of the Logit Regression on Diabetes dataset}
			\end{table}
\item Using the hand rolled LOOCV routine, the one-zero error of the logistic regression came out to be $0.7815$, the validity of this number is confirmed by utilizing the LeaveOneOut() split from the scikit.learn library, which also comes out to $0.7815$
	\end{enumerate}
\item
	\begin{enumerate}
		\item a low $\theta$ value would mean that $90\%$ of the absolute differences fall below that $\theta$ number. It reasons that a perfectly matching comparison of methods would lead to a $\theta$ of $0$, so we can conclude that a low measurement of $\theta$ implies that most ($90\%$) of the measurements are close together.
		\item The point estimate error of the statistic $\theta$ is simply the value at the index equal to $\floor*{nq}$, where $n$ is the population size and $p$ is the percentile we are interested in. This provides the sample quantile $\hat\theta$, which in the case of this data turns out to be $2.0$. We can now use boot strapping to provide the bias, standard error, and upper confidence interval: 
			\begin{table}[h]
			\centering
			\begin{tabular}{|c|c|c|}
			\hline
			\textbf{Bias} & \textbf{Standard Error} & \textbf{Upper Confidence Bound} \\
			\hline
			0.001 & 0.130& 2.200\\
			\hline
			\end{tabular}
			\caption{Bias, SE, and $95\%$ upper confidence bound for $\hat\theta$}
			\end{table}
		\item Using scipy.stats.bootstrap, we confirm that the results are accurate
			\begin{table}[h]
			\centering
			\begin{tabular}{|c|c|c|}
			\hline
			\textbf{Bias} & \textbf{Standard Error} & \textbf{Upper Confidence Bound} \\
			\hline
			0.000 & 0.130& 2.200\\
			\hline
			\end{tabular}
			\caption{Bias, SE, and $95\%$ upper confidence bound for $\hat\theta$ Using SciPy}
			\end{table}
		\item It is difficult to tell what would be an acceptable fluctuation between measurement methods without an understanding of the underlying biology. The wikipedia page tells us that the normal blood oxygen level in humans is between $96-100$, and is only considered low at below $90$. From the bootstrapping, we can say that it is likely that the two measurement types would only diverage by at most $2.2$ percent, so it is very unlikely that someone who is in critical condition would be misidentified as healthy on the field. We can have some confidence in these measurements since the bias of the $\hat\theta$ bootstrap was close to 0, which (although it doesn't gurantee it) would hint that the original sample population was unbiased. Therefore, I think that the two measurement methods would be interchangable.
	\end{enumerate}
\item
	\begin{enumerate}
		\item Using Baye's Law:
			\begin{align*}
				\frac{P(Y = 1 | X)}{P(Y=2 | X)} &= \frac{\frac{P(X|Y=1)P(Y=1)}{P(X)}}{\frac{P(X|Y=2)P(Y=2)}{P(X)}}  \\
																				&=  \frac{\frac{P(X|Y=1)\pi_1}{P(X)}}{\frac{P(X|Y=2)\pi_2}{P(X)}} \\
																				&=  \frac{P(X|Y=1)\pi_1}{P(X)}\frac{P(X)}{P(X|Y=2)\pi_2} \\ 
																				&=  \frac{P(X|Y=1)\pi_1}{P(X|Y=2)\pi_2} \\ 
			\end{align*}
		\item Using part (a) 
			\begin{align*}
				\frac{P(Y = 1 | X)}{P(Y=2 | X)} &=  \frac{P(X|Y=1)\pi_1}{P(X|Y=2)\pi_2} \\ 
				\text{log} \left (\frac{P(Y = 1 | X)}{P(Y=2 | X)} \right ) &= \text{log} \left ( \frac{P(X|Y=1)\pi_1}{P(X|Y=2)\pi_2} \right ) \\ 
																																	 &= \text{log}\frac{\pi_1}{\pi_2} \cdot \text{log} \left (  \frac{P(X|Y=1)}{P(X|Y=2)} \right )\\
			\end{align*}
			Using the LDA assumption that the conditional probality $X | Y = y$ follows a normal distribution and that all classes share a covariance matrix.
			\begin{align*}
			 \frac{P(X|Y=1)}{P(X|Y=2)} &= \frac{\exp\!\bigl(-\tfrac{1}{2}\,(x - \boldsymbol{\mu}_1)^\mathsf{T}\,\boldsymbol{\Sigma}^{-1}\,(x - \boldsymbol{\mu}_1)\bigr)}{\exp\!\bigl(-\tfrac{1}{2}\,(x - \boldsymbol{\mu}_2)^\mathsf{T}\,\boldsymbol{\Sigma}^{-1}\,(x - \boldsymbol{\mu}_2)\bigr)}, \\[6pt] \\
			 log \left ( \frac{P(X|Y=1)}{P(X|Y=2)} \right ) &= \text{log} \left ( \frac{\exp\!\bigl(-\tfrac{1}{2}\,(x - \boldsymbol{\mu}_1)^\mathsf{T}\,\boldsymbol{\Sigma}^{-1}\,(x - \boldsymbol{\mu}_1)\bigr)}{\exp\!\bigl(-\tfrac{1}{2}\,(x - \boldsymbol{\mu}_2)^\mathsf{T}\,\boldsymbol{\Sigma}^{-1}\,(x - \boldsymbol{\mu}_2)\bigr)} \right ) \\
																										 &= \text{log} \left ( \exp\Bigl(\,-\tfrac{1}{2}\,(x - \boldsymbol{\mu}_1)^\mathsf{T}\,\boldsymbol{\Sigma}^{-1}\,(x - \boldsymbol{\mu}_1)
\;+\;\tfrac{1}{2}\,(x - \boldsymbol{\mu}_2)^\mathsf{T}\,\boldsymbol{\Sigma}^{-1}\,(x - \boldsymbol{\mu}_2)\Bigr) \right ) \\
																										 &= -\tfrac{1}{2}\,(x - \boldsymbol{\mu}_1)^\mathsf{T}\,\boldsymbol{\Sigma}^{-1}\,(x - \boldsymbol{\mu}_1)
\;+\;\tfrac{1}{2}\,(x - \boldsymbol{\mu}_2)^\mathsf{T}\,\boldsymbol{\Sigma}^{-1}\,(x - \boldsymbol{\mu}_2).
			\end{align*}
			Based on this, if $\text{log}\frac{\pi_1}{\pi_2} \cdot (\tfrac{1}{2}\,(x - \boldsymbol{\mu}_1)^\mathsf{T}\,\boldsymbol{\Sigma}^{-1}\,(x - \boldsymbol{\mu}_1)\;+\;\tfrac{1}{2}\,(x - \boldsymbol{\mu}_2)^\mathsf{T}\,\boldsymbol{\Sigma}^{-1}\,(x - \boldsymbol{\mu}_2)) > 0$ we assign $X$ to class $Y=1$ otherwise we classify it as $Y=2$
	\end{enumerate}
\end{enumerate}
%% Section 2: present the code
\newpage
\begin{center}
	\textbf{Python Code}
\end{center}

\hrule
\begin{minted}{python}
	import random

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
from scipy import stats
from scipy.stats import bootstrap
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix
from sklearn.model_selection import LeaveOneOut
from tqdm import tqdm

diabetes = pd.read_csv("diabetes.csv")
diabetes = diabetes.rename(columns=lambda x: x.strip())

feature_cols = [
    "Pregnancies",
    "Glucose",
    "BloodPressure",
    "SkinThickness",
    "Insulin",
    "BMI",
    "DiabetesPedigreeFunction",
    "Age",
]
X = diabetes[feature_cols]
y = diabetes["Outcome"]

diabetes_mod = LogisticRegression(solver="lbfgs", max_iter=1000)
diabetes_mod.fit(X, y)
y_pred = diabetes_mod.predict(X)
c_matrix = confusion_matrix(y, y_pred)


def metrics(confusion_matrix):
    TP, FP, FN, TN = confusion_matrix.ravel()
    error = (FP + FN) / (TP + FP + FN + TN)
    sensitivity = TP / (TP + FN)
    specifity = TN / (TN + FP)
    return (error, sensitivity, specifity)


print(metrics(c_matrix))


##part b
def oneZeroError(prediction, row):
    actual = row.at["Outcome"]
    pred = 0 if prediction <= 0.5 else 1
    return 1 if (pred == actual) else 0


errors = []
for idx, row in tqdm(diabetes.iterrows()):
    X_test = pd.DataFrame([row[feature_cols]])
    dropped = diabetes.drop(idx)
    X = dropped[feature_cols]
    y = dropped["Outcome"]
    diabetes_mod.fit(X, y)
    prediction = diabetes_mod.predict(X_test)
    errors.append(oneZeroError(prediction, row))

print(np.mean(errors))

# Part c
X = diabetes[feature_cols]
y = diabetes["Outcome"]

loo = LeaveOneOut()

y_true, y_pred = [], []

for train_index, test_index in tqdm(loo.split(X)):
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]

    diabetes_mod.fit(X_train, y_train)

    prediction = diabetes_mod.predict(X_test)

    y_true.append(y_test.values[0])
    y_pred.append(prediction[0])

accuracy = accuracy_score(y_true, y_pred)
print(f"LOOCV Accuracy: {accuracy:.4f}")

##Question 2

blood = pd.read_csv("oxygen_saturation.txt", sep="\t")
blood["agreement"] = np.abs(blood["pos"] - blood["osm"])
# part b

# non-bootstrap estimation of the 90th quantile
quant = np.percentile(blood["agreement"], 90)
print(quant)

estimates = []
for _ in tqdm(range(100000)):
    bootstrap_sample = np.random.choice(blood["agreement"], size=72, replace=True)
    quant_estimate = np.percentile(bootstrap_sample, 90)
    estimates.append(quant_estimate)

bootstrap_quant_estimate = np.mean(estimates)
upper_ci = np.percentile(estimates, 95)
bias = bootstrap_quant_estimate - quant
se = np.std(estimates, ddof=1)
print(bias)
print(se)
print(upper_ci)
print(bootstrap_quant_estimate)


# part c
def quantile_90(x):
    return np.percentile(x, 90)


data = (np.array(blood["agreement"]).reshape(1, -1),)
res = bootstrap(
    data,
    statistic=quantile_90,
    confidence_level=0.95,
    n_resamples=100000,
    method="percentile",
    vectorized=False,
    axis=1,
)
bias = np.mean(res.bootstrap_distribution) - np.quantile(data, 0.9)
print(np.mean(res.bootstrap_distribution))
print(bias)
print(res.confidence_interval)
print(res.standard_error)

\end{minted}
\end{document}
