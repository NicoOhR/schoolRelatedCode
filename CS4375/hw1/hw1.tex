\documentclass{article} % This command is used to set the type of document you are working on such as an article, book, or presenation
\usepackage{
  amsmath,      % Math Environments
  amssymb,      % Extended Symbols
  enumerate,        % Enumerate Environments
  graphicx,      % Include Images
  lastpage,      % Reference Lastpage
  multicol,      % Use Multi-columns
  multirow      % Use Multi-rows
}


\usepackage{geometry} % This package allows the editing of the page layout
\usepackage{amsmath}  % This package allows the use of a large range of mathematical formula, commands, and symbols
\usepackage{graphicx}  % This package allows the importing of images

\newcommand{\question}[2][]{\begin{flushleft}
        \textbf{Question #1}: \textit{#2}

\end{flushleft}}
\newcommand{\sol}{\textbf{Solution}:} %Use if you want a boldface solution line
\newcommand{\maketitletwo}[2][]{\begin{center}
        \Large{\textbf{Assignment #1}
            
            Machine Learning} % Name of course here
        \vspace{5pt}
        
        \normalsize{N. Ohayon Rozanes  % Your name here
        
        \today}        % Change to due date if preferred
        \vspace{15pt}
        
\end{center}}
\begin{document}
\maketitletwo[1]{}
    \question[1]{ }
    \begin{enumerate}
      \item
        $$
        L_i(b, w) = |w^Tx + b - y|^3
        $$
    A feature of this loss function, which is similar to the square aboslute error loss function, is that any error $> 1$ will be magnified, while error $< 1$ is discounted. That is to say that "large" misses are catastrophic to the loss when compared to absolute or square absolute error. This is sometimes helpful if the data is well normalized, or if the importance of errors does not scale linearly due to some domain specific reasons, but can cause issues, such as two errors which are quite close (for example the error at $x_i = 0.9$ and $x_j = 0.8$) contributing very different amounts to the gradient decent. This can also be an issue when there are outliers present in the data set, causing this loss function to be less robust than the square absolute loss. The absolute cube function maintains a couple of nice properties for a loss function. Namely, it is convex ($L'' \ge 0$), is regular around $0$ (smooth, continuous). 


Clearly, $L_i$ is at least $\in C^1[\mathbb{R}^n \times \mathbb{R}^m]$, where $d$ is the dimension of input and $m$ is the dimension of the output, I assume $m = 1$, but the analysis changes only a little in higher dimensions, swapping the absolute value function for the induced norm $|| \cdot ||$ in $\mathbb{R}^m$.The loss function is then continously differentiable twice with respect to both the $w$ and $b$ parameters. We show this by explicitly computing the partial derivatives with resepct to $w$ and $b$, which we can use to compute the gradient for part (b) as well:

    \begin{align*}
      L_i &= |w^Tx^i + b - y|^3 \\
      L_i &= \left((w^Tx^i + b - y)^2\right)^\frac{3}{2}
    \end{align*}

    Then:
    \begin{align*}
      \frac{\partial L_i}{\partial w_n} &= \frac{3}{2}\left((w^Tx^i + b - y)^2\right)^\frac{1}{2}2(w^Tx^i+b-y)(x^i_n)\\ 
                                      &= 3|w^Tx^i+b-y|(w^Tx^i+b-y)(x^i_n) \\ 
    \end{align*}
    Similarily
    \begin{align*}
      \frac{\partial L_i}{\partial b} &= \frac{3}{2}\left((w^Tx + b - y)^2\right)^\frac{1}{2}2(w^Tx+b-y)(1)\\ 
                                      &= 3|w^Tx+b-y|(w^Tx+b-y)\\ 
    \end{align*}

Although not explicitly relevent, because we plan to use gradient decent, there is no closed form optimizer for this loss function due to complications with the absolute value causing the derivatives to depend on the sign of the error. The gradient of this loss function is presented below
$$
\nabla L_i = 
\begin{pmatrix}
3|w^Tx^i+b-y^i|(w^Tx^i+b-y^i)(x^i_0) \\ 
3|w^Tx^i+b-y^i|(w^Tx^i+b-y^i)(x^i_1) \\ 
\hdots\\
3|w^Tx^i+b-y^i|(w^Tx^i+b-y^i)(x^i_n) \\ 
3|w^Tx^i+b-y^i|(w^Tx^i+b-y^i)
\end{pmatrix}
$$

\item 
  $$
  L_i = [w^Tx^i + b - y^i]^3
  $$
  This loss function is out of hand unusable because it is not not bounded below. This can be demonstrated trivially by inspecting the behavior of the loss function as $b\to-\infty$
  \[
    \lim_{b\to\-\infty} L_i(w, b) = -\infty
  \]

  A gradient decent algorithm would diverge to $-\infty$ as it would nudge the parameters away from the data points in the negative direction. Although less relevent, there's also the issue that the function is not convex, which means that we can not guratnee a global minimum. A derivation of the gradient of this loss function follows: 
  \begin{align*}
    \frac{\partial L_i}{\partial w_n} &= 3(w^Tx^i + b - y_i)^2(x^i_n) \\
    \frac{\partial L_i}{\partial b} &= 3(w^Tx^i + b - y_i)^2 
  \end{align*}
  Thus 
  \[
    \nabla L_i = \begin{pmatrix}
      pmatrix
    \end{pmatrix}
  \]
\item
  \[
    L_i(w,b) = \exp[w^Tx^i + b - y^i]
  \]

\end{enumerate}
    
    
    \question[2]{Here is my second question}
    
    YOUR SOLUTION HERE
    
    \question[3]{What is the \Large{$\int_0^2 x^2 \, dx $}\normalsize{. Show all steps}}
    
    \begin{align*}
    \int_0^2 x^2 &= \left. \frac{x^3}{3} \right|_0^2 \\
                 &= \frac{2^3}{3}-\frac{0^3}{3}\\
                 &= \frac{8}{3}
    \end{align*}
\end{document}
