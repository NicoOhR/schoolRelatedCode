\documentclass[12pt]{article}

\usepackage[utf8]{inputenc}
\usepackage{latexsym,amsfonts,amssymb,amsthm,amsmath}

\setlength{\parindent}{0in}
\setlength{\oddsidemargin}{0in}
\setlength{\textwidth}{6.5in}
\setlength{\textheight}{8.8in}
\setlength{\topmargin}{0in}
\setlength{\headheight}{18pt}



\title{Homework 4}
\author{N. Ohayon Rozanes}

\begin{document}

\maketitle


\subsection*{Exercise 1}
\begin{proof}
  multiplying out the terms and simplifying we get that
  \begin{align*}
    (I - H)(I - H) &= I - 2H + HH 
  \end{align*}
  However, since $H$ is a project matrix and therefore idempotent, we can say that $HH = H$ so: 
  \begin{align*}
    I - 2H + HH &= I - 2H + H\\
                &= I - H
  \end{align*}
  Thererfore we have proven that $(I - H)^2 = (I - H)$ is idempotent
\end{proof}
\subsection*{Exercise 2}
\begin{proof}
  Clearly
  \begin{align*}
    \operatorname{Var}(\mathbf{A}y) &= \operatorname{E}[\mathbf{A}y - \operatorname{E}(\mathbf{A}y)](\mathbf{A}y - \operatorname{E}[\mathbf{A}y]))' \\
            &= \operatorname{E}[\mathbf{A}(y - \operatorname{E}(y))](\mathbf{A}(y - \operatorname{E}[y])) \\
            &= \mathbf{A}\operatorname{E}[(y - \operatorname{E}(y))](y - \operatorname{E}[y]))'\mathbf{A}' \\
            &= \mathbf{A}\operatorname{Var}(y)\mathbf{A}' \\
  \end{align*}
\end{proof}
\subsection*{Exercise 3}
\begin{enumerate}
  \item For the simple regression case, $\mathbf{X}$ is given by: 
    \[
      \mathbf{X} = 
      \begin{bmatrix}
        1 & x_{1} \\
        1 & x_{2} \\ 
        1 & x_{3} \\ 
        \vdots & \vdots \\
        1 & x_n
      \end{bmatrix}
    \]
    So clearly: 
    \[
      \mathbf{X'X} = \begin{bmatrix} \mathbf{1}'\cdot\mathbf{1}  & \mathbf{1}'\cdot\mathbf{x}\\
        \mathbf{x}'\cdot\mathbf{1} & \mathbf{x}'\cdot\mathbf{x}
      \end{bmatrix}
    \]
    Expressed using summations we get that 
      
    \[
      \mathbf{X'X} = \begin{bmatrix} n  & \sum x_i\\
        \sum x_i & \sum x_i^2 
      \end{bmatrix}
    \]
    Using the standard inversion formula for a $2\times 2$ matrix,
    \[
      (\mathbf{X'X})^{-1} = \frac{1}{n\sum x_i^2 - (\sum x_i)^2}
      \begin{bmatrix}
        \sum x_i^2 & \sum x_i \\ \sum x_i & n
      \end{bmatrix}
    \]
    To simplify the fraction a little 
    \begin{align*}
      \frac{1}{n\sum x_i^2 - (\sum x_i)^2} &= \frac{1}{n\sum x_i^2 - (n\bar x)^2} \\
                                           &= \frac{1}{n(\sum x_i^2 - n(\bar x)^2)} \\
                                           &= \frac{1}{nS_{xx}}
    \end{align*}
    Then multiply by the right and left by $\mathbf{X'}$ and $\mathbf{X}$ to get the hat matrix $\mathbf{H}$. It's a bit easier to express in vector form, take $\mathbf{x}$ to be the data vector and $\mathbf{1}$ to be the all $1$ vector
\begin{align*}
\mathbf{X(X'X)^{-1}X'} 
&= 
\begin{bmatrix}\mathbf{1} & \mathbf{x}\end{bmatrix}
\left(
\frac{1}{nS_{xx}}
\begin{bmatrix}
\mathbf{x'x} & -\mathbf{1'x}\\[2pt]
-\mathbf{1'x} & n
\end{bmatrix}
\right)
\begin{bmatrix}\mathbf{1}\\ \mathbf{x}\end{bmatrix}
\\[6pt]
&= \frac{1}{nS_{xx}}
\begin{bmatrix}
\mathbf{1} & \mathbf{x}
\end{bmatrix}
\begin{bmatrix}
\mathbf{x'x} & -\mathbf{1'x}\\[2pt]
-\mathbf{1'x} & n
\end{bmatrix}
\begin{bmatrix}\mathbf{1}\\ \mathbf{x}\end{bmatrix}
\\[6pt]
&= \frac{1}{nS_{xx}}
\Big(
(\mathbf{x'x})\,\mathbf{1}\mathbf{1}'
-(\mathbf{1'x})\,\mathbf{x}\mathbf{1}'
-(\mathbf{1'x})\,\mathbf{1}\mathbf{x}'
+ n\,\mathbf{x}\mathbf{x}'
\Big)
\\[6pt]
&= \frac{1}{nS_{xx}}
\Big(
(n\bar{x}^2+S_{xx})\,\mathbf{1}\mathbf{1}'
- n\bar{x}\,\mathbf{x}\mathbf{1}'
- n\bar{x}\,\mathbf{1}\mathbf{x}'
+ n\,\mathbf{x}\mathbf{x}'
\Big)\\
&= \frac{1}{S_{xx}}
\Big(
\mathbf{x}\mathbf{x}' - \bar{x}\,\mathbf{x}\mathbf{1}' - \bar{x}\,\mathbf{1}\mathbf{x}' + \bar{x}^2\,\mathbf{1}\mathbf{1}'
\Big)
\;+\;
\frac{1}{n}\,\mathbf{1}\mathbf{1}'
\\[6pt]
&= \frac{1}{n}\, \;+\; 
\frac{(\mathbf{x}-\bar{x}\mathbf{1})\,(\mathbf{x}-\bar{x}\mathbf{1})'}{S_{xx}}.
\end{align*}
\item The more extreme values of the data vector $\mathbf{x}$, the values further from $\bar x$ will yield a value larger in magnitude in the numerator. That is to say, that the data pints further from the mean will exert a larger influence on the projection.
\end{enumerate}
\subsection*{Exercise 4}
This intuitvely makes sense, since the true $y$ can be decomposed to the parallel component $X\beta$ and the orthogonal component $\varepsilon$, regardless, notice that:
\begin{align*}
  \hat\beta &= \beta + (X'X)^{-1}X'\varepsilon \\
  (X'X)^{-1}X'y &= \beta + (X'X)^{-1}X'\varepsilon \\
  (X'X)(X'X)^{-1}X'y &= (X'X)\beta + (X'X)(X'X)^{-1}X'\varepsilon \\
  X'y &= (X'X)\beta + X'\varepsilon\\
  (X')^{-1}X'y &= (X')^{-1})(X'X)\beta + (X')^{-1}X'\varepsilon \\
  y &= X\beta + \varepsilon
\end{align*}
Which is true by the definition of the linear regression problem statement.
\subsection*{Exercise 5}

\begin{align*}
  e  &= (I - H)\varepsilon  \\
  (I - H)y &= (I - H)\varepsilon \\
  (I - H)(X\beta + \varepsilon) &= (I - H)\varepsilon \\
  IX\beta - HX\beta + (I - H)\varepsilon &= (I - H)\varepsilon \\
  X\beta - X\beta + (I - H)\varepsilon &= (I - H)\varepsilon \\
  (I - H)\varepsilon &= (I - H)\varepsilon
\end{align*}
\subsection*{Exercise 6}

\begin{align*}
  E[\hat\beta_1] &= E[(X_1'X_1)^{-1}X_1'y] \\ 
                 &= E[(X_1'X_1)^{-1}X_1'(X_1\beta_1 + X_2\beta_2 + \varepsilon)] \\
                 &= E[(X_1'X_1)^{-1}X_1'X_1\beta_1 + (X'_1X_1)^{-1}X'_1X_2\beta_2 + (X'_1X_1)^{-1}X'_1\varepsilon] \\ 
                 &= E[(X_1'X_1)^{-1}X_1'X_1\beta_1] + E[(X'_1X_1)^{-1}X'_1X_2\beta_2] + E[(X'_1X_1)^{-1}X'_1\varepsilon] \\ 
                 &= \beta_1 + (X_1'X_1)^{-1}(X'_1)X_2\beta_2 + 0
\end{align*}

Clearly, this estimator is unbiased only when $\beta_2$ is in the null space of $(X_1'X_1)^{-1}X_1'X_2$, although since $(X_1'X_1)^{-1}$ is obviously invertible, it has a trivial null space so the estimator is unbiased when $X_1'X_2\beta_2 = 0$. Since both $X_1$ and $X_2$ are assumed to be full rank (no co-linearity), then the null space is only non-trivial when the columns of $X_1$ are orthogonal to the columns of $X_2$, in which case the null space is the all of $\mathbb{R}^n$. Therefore, the only time the estimator is unbiased is when $X_1$ and $X_2$ have orthogonal columns.

\subsection*{Exercise 7}
\begin{align*}
  \sum Var(\hat y_i) &= tr(Var(\hat y))\\
  &= tr(Var(Hy)) \\ 
  &= tr(H'Var(y)H) \\
  &= tr(H'(\sigma^2I)H) \\
  &= \sigma^2tr(H'H) \\
  &= \sigma^2tr(H) \\ 
  &= \sigma^2tr(X(X'X)^{-1}X') \\ 
\end{align*}
since the trace of a product of matrices commutes, that is $tr(AB) = tr(BA)$, we get that 
\[
  \sigma^2tr(X(X'X)^{-1}X') = \sigma^2tr(X'X(X'X)^{-1})
\]
$X'X$ is a square matrix with dimensionality of the number of columns (predictors) of the $X$ matrix, so the product in the trace comes out to $I_p$
\[
  \sigma^2tr(X'X(X'X)^{-1}) = \sigma^2tr(I_p) = \sigma^2p
\]
\end{document}
