\documentclass[12pt]{article}

\usepackage[utf8]{inputenc}
\usepackage{latexsym,amsfonts,amssymb,amsthm,amsmath}
\usepackage{physics}
\setlength{\parindent}{0in}
\setlength{\oddsidemargin}{0in}
\setlength{\textwidth}{6.5in}
\setlength{\textheight}{8.8in}
\setlength{\topmargin}{0in}
\setlength{\headheight}{18pt}

\renewcommand{\labelenumi}{(\alph{enumi})}
\newcommand*{\pd}[3][]{\ensuremath{\frac{\partial^{#1} #2}{\partial #3}}}
\renewcommand{\epsilon}{\varepsilon}

\title{STAT 4355}
\author{N. Ohayon Rozanes}

\begin{document}

\maketitle

\vspace{0.5in}



\subsection*{Exercise 1}
\begin{enumerate}
    \item 
        \begin{proof}
            \begin{align*}
                S_{yy} &= \sum (y_i - \bar{y})^2 \\
                       &= \sum (y_i^2 - 2y_i\bar{y} + \bar{y}^2) \\ 
                       &= \sum (y_i^2 -2y_i\bar y) +  n\bar  y^2 \\
                       &= \sum(y_i^2) - 2\bar y \sum y_i + n\bar y^2 \\
                       &= \sum(y_i^2)  + \bar y n(-\frac2n\sum y_i + \bar y) \\
                       &= \sum(y_i^2) + \bar y n(-2\bar y + \bar y) \\
                       &= \sum (y_i^2) - n\bar y ^2  
            \end{align*}
        \end{proof}
    \item 
        Define $\hat x  := \sum x_i$
        \begin{proof}
            \begin{align*}
                S_{xy} &= \sum (x_i - \bar x )(y_i - \bar y) \\
                       &= \sum (x_iy_i - \bar xy_i -\bar yx_i + \bar x \bar y) \\
                       &= \sum x_iy_i - \bar x\hat y  -\bar y \hat x + n \bar x \bar y \\
                       &= \sum x_iy_i - \frac1n(\hat x\hat y  -\hat y \hat x + n^2\bar x \bar y) \\
                       &= \sum x_iy_i - (n\bar x \bar y) \\
                       &= \sum x_iy_i - (\frac{\hat x  \hat y}{n})\\
                       &= \sum x_iy_i - (\frac{\sum x_i  \sum y_i}{n})\\
            \end{align*}     
        \end{proof}
\end{enumerate}

\vspace{2in} %Leave space for comments!


\subsection*{Exercise 2}
\begin{enumerate}
    \item 
        \begin{proof}
        \begin{align*}
            Var(cX) &= E[(cX - E(cX))^2] \\
                    &= E[(cX)^2 - 2cXE(cX) + E(cX)^2] \\
                    &= E[(cX)^2] -2E[cX]E[cX] + E[cX]^2 \\
                    &= E[(cX)^2] - E[cX]^2 \\
                    &= c^2E[X^2] - c^2E[X]^2 \\ 
                    &= c^2(E[X^2] - E[X]^2) \\
                    &= c^2(Var[X])
        \end{align*}
        \end{proof}
    \item 
        \begin{proof}
            \begin{align*}
                Var(aX + bY) &= E[(aX + bY)^2] - E[aX + bY]^2 \\
                             &= E[a^2X^2 + 2abXY + b^2Y^2] - (E[aX] + E[bY])^2 \\
                             &= a^2E[X^2] + 2abE[XY] + b^2E[Y^2] - E[aX]^2 -2E[aX]E[bX] - E[bY]^2 \\
                             &= a^2E[X^2] + 2abE[XY] + b^2E[Y^2] -a^2E[X]^2 -2abE[X]E[Y]- b^2E[Y]^2 \\
                             &= a^2(E[X^2] - E[X]^2) + b^2(E[Y^2] - E[Y]^2) + 2ab(E[XY] -E[X]E[Y]) \\
                             &= a^2Var(X) + b^2Var(Y) + 2abCov(X,Y)
            \end{align*}
            The last step $2ab(E[XY] - E[X]E[Y]) = 2abCov(X, Y)$ is justified by the result in part (c)
        \end{proof}
    \item  
        \begin{proof}
            \begin{align*}
                Cov(X,Y)  &:= E[(X-E[X])(Y-E[Y])] \\
                          &= E[XY -XE[Y] -YE[X] + E[X]E[Y]]\\ 
                          &= E[XY] - E[XE[Y]] -E[YE[X]] + E[E[X]E[Y]] \\
                          &= E[XY] - E[X]E[Y] - E[Y]E[X] + E[X]E[Y] \\ 
                          &= E[XY] - E[X]E[Y]
            \end{align*}
        \end{proof}
    \item Using the result from part (c): $$Cov(aX + bY, Z) = E[(aX + bY)Z] - E[aX + bY]E[Z]$$
        Therefore:
    \begin{align*}
        E[(aX + bY)Z] - E[aX + bY]E[Z] &= E[aXZ + bYZ] - E[aX]E[Z] - E[bY]E[Z] \\
                                       &= aE[XZ] + bE[YZ] -aE[X]E[Z] - bE[Y]E[Z] \\
                                       &= a(E[XZ] - E[X]E[Z]) + b(E[YZ] - E[YZ]) \\
                                       &= aCov(X, Z) + bCov(Y,Z)
    \end{align*}
\end{enumerate}
\subsection*{Question 3}
\begin{enumerate}
    \item  
    \[
        S(\beta_1) = \sum^n_{i=1}(y_i - \beta_0 - \beta_1x_i)^2
    \]
    \item 
    \[
        \pdv{S}{\beta_1} = -2 \sum_{i=1}^n(y_i - \hat \beta_0 - \beta_1x_i)x_i
    \]
    Setting to 0 to optimize, then simplify to obtain the normal equation
    \begin{align*}
            0 &=  \sum_{i=1}^n(y_i - \hat \beta_0 - \beta_1x_i)x_i \\
              &= \sum y_ix_i - \hat\beta_0x_i - \hat \beta_1x_i^2 \\
              &= \sum y_ix_i - \hat\beta_0\sum x_i - \hat\beta_1\sum x_i^2 \\
    \end{align*}
\item following immediatly from the normal equation 
    \begin{align*}
            0 &= \sum y_ix_i - \hat\beta_0\sum x_i - \hat\beta_1\sum x_i^2 \\
            \hat\beta_1\sum x_i^2 &= \sum y_ix_i - \hat\beta_0\sum x_i  \\
            \hat\beta_1 &= \frac{\sum y_ix_i - \hat\beta_0\sum x_i}{\sum x_i^2} \\
    \end{align*}
\item 
    \begin{align*}
        E[\hat \beta_1] &= E[ \frac{\sum y_ix_i - \beta_0\sum x_i}{\sum x_i^2} ]\\
                        &= E[\frac{\sum y_ix_i}{\sum x_i^2}] - \beta_0E[\frac{\sum x_i}{\sum x_i^2}]\\ 
                        &= E[\frac{\sum y_ix_i}{\sum x_i^2}] - \beta_0\frac{\sum x_i}{\sum x_i^2} \\ 
                        &= \frac1{\sum x_i^2}\sum x_iE[y_i] - \beta_0\frac{\sum x_i}{\sum x_i^2} \\ 
                        &= \frac1{\sum x_i^2}(\sum x_iE[y_i] - \beta_0{\sum x_i}) \\ 
                        &= \frac1{\sum x_i^2}(\sum x_i(\beta_0 + \beta_1x_i) - \beta_0{\sum x_i}) \\ 
                        &= \frac1{\sum x_i^2}(\beta_0\sum x_i+ \beta_1\sum x_i^2 - \beta_0{\sum x_i}) \\ 
                        &= \frac1{\sum x_i^2}(\beta_1\sum x_i^2) \\ 
                        &=  \beta_1
    \end{align*}
\item 
    \begin{align*}
        Var[\hat\beta_1]  &=  Var[ \frac{\sum y_ix_i - \beta_0\sum x_i}{\sum x_i^2} ]\\
                          &= Var[ \frac{\sum y_ix_i}{\sum x_i^2} + (-\beta_0\frac{\sum x_i}{\sum x^2_i})]\\ 
                          &= \frac1{(\sum x_i^2)^2}Var[{\sum y_ix_i}] + \beta_0^2Var[\frac{\sum x_i}{x_i^2}] + 2\frac{\beta_0}{\sum x_i^2}Cov(\sum y_ix_i, \frac{\sum x_i}{x_i^2}) \\ 
    \end{align*}
    Notice, only the first term is non-constant, therefore:
    $$
    Var[\hat\beta_1]= \frac1{(\sum x_i^2)^2}Var[{\sum y_ix_i}] 
    $$
    And because $y_i$ obersvations are assumed to be independent, the covariance terms fall out, so the above summation can be rewritten as:
    $$
    Var[\hat\beta_1]= \frac{1}{(\sum x_i^2)^2}\sum x_i^2 Var(y_i) 
    $$
    Since $y_i = \beta_0 + \beta_1 xi + \varepsilon$, the variance of $y_i$ is exactly $Var(\varepsilon)$ which we assume to be $\sigma^2$ 
    \begin{align*}
        Var[\hat\beta_1]&= \frac{\sigma^2\sum x_i^2}{(\sum x_i^2)^2} \\
        &= \frac{\sigma^2}{\sum x_i^2}
    \end{align*}
\end{enumerate}
\subsection*{exercise 4}
\begin{enumerate}
    \item 
\begin{align*}
    \sum (y_i - \bar y)^2 &= \sum ((\beta_0 + \beta_1x_i + \varepsilon_i) - (\beta_0 + \beta_1\bar x + \bar \epsilon))^2 \\
                          &= \sum (\beta_1(x_i - \bar x) + (\epsilon_i - \bar\epsilon))^2\\
                          &= \sum (\beta_1(x_i - \bar x))^2 + 2\beta_1(x_i - \bar x)(\epsilon_i - \bar\epsilon) + (\epsilon_i - \bar\epsilon)^2 \\
                          &= \beta_1^2\sum(x_i - \bar x)^2 + 2\beta_1\sum(x_i - \bar x)(\epsilon_i - \bar\epsilon) + \sum(\epsilon_i - \bar\epsilon)^2 \\
                          &= \beta_1^2S_{xx}+ 2\beta_1\sum(x_i - \bar x)(\epsilon_i - \bar\epsilon) + \sum(\epsilon_i - \bar\epsilon)^2
\end{align*}
\item 
    \begin{align*}
        E[S_{yy}] &= E[\beta_1^2S_{xx}+ 2\beta_1\sum(x_i - \bar x)(\epsilon_i - \bar\epsilon) + \sum(\epsilon_i - \bar\epsilon)^2] \\
                  &=  \beta_1^2E[S_{xx}] + 2\beta_1E[\sum(x_i - \bar x)(\epsilon_i - \bar\epsilon)] + E[\sum(\epsilon_i - \bar\epsilon)^2]] \\
                  &= \beta_1^2S_{xx} + 2\beta_1\sum(x_i - \bar x)E[\epsilon_i - \bar\epsilon] + E[\sum (\epsilon_i - \bar\epsilon)^2]\\
                  &= \beta_1^2S_{xx} + \frac{n-1}{n-1}E[\sum (\epsilon_i - \bar\epsilon)^2] \\
                  &= \beta_1^2S_{xx} + (n-1)E[\sum (\frac{\epsilon_i - \bar\epsilon)^2}{n-1}]\\
                  &= \beta_1^2S_{xx} + (n-1)\sigma^2 
    \end{align*}
\item  Given that $Var[X] = E[X^2] - E[X]^2 \implies E[X^2] = Var[X] + E[X]^2$ 
    $$
        E[\hat\beta_1^2]  = Var[\hat\beta_1] + E[\hat\beta_1^2]
    $$
    Using the results from question 2, part (d) and (e), $Var[\hat\beta_1] = \frac{\sigma^2}{\sum x_i^2}$ and $E[\hat\beta_1] = \beta_1$
    $$  
    E[\hat\beta_1^2] = \frac{\sigma^2}{\sum x_i^2} + \hat\beta_1^2
    $$
\item 
\begin{align*}
    E[\frac{SSE}{n-2}] &= E[\frac{S_{yy} - \hat\beta_1^2S_{xx}}{n-2}]\\
                       &= E[\frac{S_{yy}}{n-2}]   - E[\frac{\hat\beta_1^2S_{xx}}{n-2}] \\ 
                       &= \frac1{n-2}(\beta_1^2S_{xx}  + (n-1)\sigma^2 - S_{xx}E[\beta_1^2])\\
                       &= \frac1{n-2}(\beta_1^2S_{xx}  + (n-1)\sigma^2 - S_{xx}(\frac{\sigma^2}{S_{xx}}) - S_{xx}\beta_1^2)\\
                       &= \frac1{n-2}(\beta_1^2S_{xx}  + (n-1)\sigma^2 - \sigma^2 - S_{xx}\beta_1^2)\\
                       &= \frac1{n-2}((n-2)\sigma^2)\\
                       &= \sigma^2
\end{align*}
\end{enumerate}
\end{document}

